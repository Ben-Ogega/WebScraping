{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5c9cd9-ba7f-4fca-bb8f-23f951fd0ef2",
   "metadata": {},
   "source": [
    "<img \n",
    "    style=\"display: block; \n",
    "           margin-left: auto;\n",
    "           margin-right: auto;\n",
    "           width: 100%;\"\n",
    "    src=\"./img/The-Simple-Steps-for-Web-Scraping (1).jpg\"/>\n",
    "\n",
    "</img>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6b5d9c-67d2-4d60-a713-0758646341de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary Python Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0314d-1713-48b2-a1f2-fddcf55438ae",
   "metadata": {},
   "source": [
    "#### 1. Get the list of topics from the topics page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbc2d60-9891-4e8f-b83a-3f710fd6846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url = 'https://github.com/topics'\n",
    "response = requests.get(topics_url)\n",
    "# page_contents = response.text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd01434a-95aa-4f25-8e99-462e71b63815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(topics_url)\n",
    "# # page_contents = response.text      //Uncomment this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680ce3c-4b3e-47c3-b81e-43f0a24229bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_html_to_file_and_parse(page_contents, file_path='webpage.html'):\n",
    "    \"\"\"\n",
    "    Write HTML content to a file and parse it using BeautifulSoup.\n",
    "\n",
    "    Parameters:\n",
    "    - page_contents (str): The HTML content to be written to the file and parsed.\n",
    "    - file_path (str): The path to the HTML file (default is 'webpage.html').\n",
    "\n",
    "    Returns:\n",
    "    - BeautifulSoup object: Parsed HTML content using BeautifulSoup.\n",
    "\n",
    "    Example:\n",
    "    html_content = get_html_content_from_source()\n",
    "    soup = write_html_to_file_and_parse(html_content, 'output.html')\n",
    "    print(soup.title)\n",
    "    \"\"\"\n",
    "    # Write the HTML content to a file\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(page_contents)\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_contents, 'html.parser')\n",
    "\n",
    "    return soup\n",
    "\n",
    "# Example Usage:\n",
    "# html_content = get_html_content_from_source()\n",
    "# soup = write_html_to_file_and_parse(html_content, 'output.html')\n",
    "# print(soup.title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f5a51-75db-4745-b135-f87d2537f4d2",
   "metadata": {},
   "source": [
    "## 1. Get The List Of Topics From The Topics Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab0cdf-7e70-4244-be6a-8a8fb5fe15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(soup):\n",
    "    \"\"\"\n",
    "    Extract topic titles from a BeautifulSoup object representing a GitHub topics page.\n",
    "\n",
    "    Parameters:\n",
    "    - soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list of topic titles extracted from the HTML.\n",
    "\n",
    "    Example:\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    titles = get_topic_titles(soup)\n",
    "    print(titles)\n",
    "    \"\"\"\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = soup.find_all('p', {'class': selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282abf5-0e94-4a78-bd49-c2232241957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_titles(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8395b-1fc4-45a6-beff-48e5df6bcbc9",
   "metadata": {},
   "source": [
    "## 2. Get The List Of Top Repos From The Individual Topic Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb72f89-66c2-49ae-9b5b-ebbfc573e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(soup):\n",
    "    \"\"\"\n",
    "        Extract topic descriptions from a BeautifulSoup object representing a GitHub topics page.\n",
    "        \n",
    "        Parameters:\n",
    "        - soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML.\n",
    "        \n",
    "        Returns:\n",
    "        - list of str: A list of topic descriptions extracted from the HTML.\n",
    "        \n",
    "        Example:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        descriptions = get_topic_descs(soup)\n",
    "        print(descriptions)\n",
    "    \"\"\"\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    # soup.find('div', {\"class\":\"stars\"}) ['title']\n",
    "    topic_desc_tags = soup.find_all('p', {'class': desc_selector})\n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47572a9-6f1e-4fca-b1c1-33cf551d0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_descs(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6d44a-5ce7-4b9f-9ecc-1243e3f6c857",
   "metadata": {},
   "source": [
    "## 3. For Each Topic, Create A CSV Of The Top Repos For The Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623674fe-4035-4ece-a996-a72af22c009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(soup):\n",
    "    \"\"\"\n",
    "    Extract topic URLs from a BeautifulSoup object representing a GitHub topics page.\n",
    "\n",
    "    Parameters:\n",
    "    - soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list of topic URLs extracted from the HTML.\n",
    "\n",
    "    Example:\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    urls = get_topic_urls(soup)\n",
    "    print(urls)\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    topic_link_tags = soup.find('div', class_='col-lg-9 position-relative pr-lg-5 mb-6 mr-lg-5')\n",
    "\n",
    "    if topic_link_tags:\n",
    "        # Find the a tag within the a tag\n",
    "        a_tags = topic_link_tags.find_all('a', class_=\"no-underline flex-grow-0\")\n",
    "     \n",
    "        topic_urls = []\n",
    "        base_url = 'https://github.com'\n",
    "        for tag in a_tags:\n",
    "            topic_urls.append(base_url + tag.get('href'))\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26084d20-7cf5-450a-b967-ddab1e70deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topic_urls(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b1779-fc72-42dc-8dae-1bbe298d4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    \"\"\"\n",
    "    Scrape GitHub topics information from the GitHub topics page.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing information about GitHub topics, including title, description, and URL.\n",
    "\n",
    "    Example:\n",
    "    topics_data = scrape_topics()\n",
    "    print(topics_data)\n",
    "    \"\"\"\n",
    "    # URL of the GitHub topics page\n",
    "    topics_url = 'https://github.com/topics'\n",
    "\n",
    "    # Send an HTTP GET request to the topics URL\n",
    "    response = requests.get(topics_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topics_url))\n",
    "\n",
    "    # Create a BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract information about topics using helper functions\n",
    "    topics_dict = {\n",
    "        'Title': get_topic_titles(soup),\n",
    "        'Description': get_topic_descs(soup),\n",
    "        'URL': get_topic_urls(soup)\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    topics_df = pd.DataFrame.from_dict(topics_dict)\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "# Example Usage:\n",
    "# topics_data = scrape_topics()\n",
    "# print(topics_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79ad5d-e3e7-455b-8b7e-c8c1591eb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_topics().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d295c4d-883d-4331-ab32-54c07ddc89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    \"\"\"\n",
    "    Scrape information about top repositories for GitHub topics and print URLs.\n",
    "\n",
    "    This function prints the URLs and file paths for storing data related to top repositories for GitHub topics.\n",
    "\n",
    "    Example:\n",
    "    scrape_topics_repos()\n",
    "    \"\"\"\n",
    "    print('Scraping list of topics')\n",
    "\n",
    "    # Call the scrape_topics function to get information about GitHub topics\n",
    "    topics_df = scrape_topics()\n",
    "\n",
    "    # Check if scrape_topics returned None\n",
    "    if topics_df is None:\n",
    "        print(\"Error: scrape_topics() returned None.\")\n",
    "        return\n",
    "\n",
    "    # Create a directory named 'data' if it doesn't exist\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "\n",
    "    # Iterate through the rows of the topics DataFrame\n",
    "    for index, row in topics_df.iterrows():\n",
    "        # Print the URL and file path for each topic\n",
    "        print(row['URL'], ' data/{}.csv'.format(row['Title']))\n",
    "\n",
    "# Example Usage:\n",
    "# scrape_topics_repos()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a985030-924b-4599-b4f8-be6dcce99e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f8026-e5e4-4409-8b36-66f260b0258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path\n",
    "folder_path = 'data'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "def save_topics_to_csv():\n",
    "    \"\"\"\n",
    "    Save information about GitHub topics to a CSV file.\n",
    "\n",
    "    This function creates a folder named 'data' if it doesn't exist, specifies the file path,\n",
    "    and saves the DataFrame containing information about GitHub topics to a CSV file.\n",
    "\n",
    "    Example:\n",
    "    save_topics_to_csv()\n",
    "    \"\"\"\n",
    "    # Specify the file path (joining the folder path and file name)\n",
    "    file_path = os.path.join(folder_path, 'Top GitHub repositories.csv')\n",
    "\n",
    "    # Call the scrape_topics function to get information about GitHub topics\n",
    "    topics_df = scrape_topics()\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    topics_df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f'DataFrame saved to {file_path}')\n",
    "\n",
    "# Example Usage:\n",
    "# save_topics_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79e623-2d62-4c03-9eac-34ba9b950a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_topics_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884911eb-6146-4681-b260-26c5b2fd319b",
   "metadata": {},
   "source": [
    "## Future Undertaking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc56ac8-d973-4757-b53a-a6b1b035cce3",
   "metadata": {},
   "source": [
    "### We can modify the code to include the star ratings for individual repositories under each topic and save the data to individual CSV files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0eea6-7313-490a-b2dc-3fb5e611f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_topics():\n",
    "    \"\"\"\n",
    "    Scrape GitHub topics information from the GitHub topics page.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing information about GitHub topics, including title, description, and URL.\n",
    "\n",
    "    Example:\n",
    "    topics_data = scrape_topics()\n",
    "    print(topics_data)\n",
    "    \"\"\"\n",
    "    # URL of the GitHub topics page\n",
    "    topics_url = 'https://github.com/topics'\n",
    "\n",
    "    # Send an HTTP GET request to the topics URL\n",
    "    response = requests.get(topics_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topics_url))\n",
    "\n",
    "    # Create a BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract information about topics using helper functions\n",
    "    topics_dict = {\n",
    "        'Title': get_topic_titles(soup),\n",
    "        'Description': get_topic_descs(soup),\n",
    "        'URL': get_topic_urls(soup)\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    topics_df = pd.DataFrame.from_dict(topics_dict)\n",
    "\n",
    "    # Add a new column to store the individual repository data\n",
    "    topics_df['Repositories'] = topics_df['URL'].apply(scrape_topic_repositories)\n",
    "\n",
    "    return topics_df\n",
    "\n",
    "def scrape_topic_repositories(topic_url):\n",
    "    \"\"\"\n",
    "    Scrape information about individual repositories under a GitHub topic.\n",
    "\n",
    "    Parameters:\n",
    "    - topic_url (str): The URL of the GitHub topic.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing information about individual repositories, including subtopic, repository name, user name, and star ratings.\n",
    "\n",
    "    Example:\n",
    "    repositories_data = scrape_topic_repositories('https://github.com/topics/python')\n",
    "    print(repositories_data)\n",
    "    \"\"\"\n",
    "    # Send an HTTP GET request to the topic URL\n",
    "    response = requests.get(topic_url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "\n",
    "    # Create a BeautifulSoup object from the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract information about individual repositories using helper functions\n",
    "    repositories_dict = {\n",
    "        'Subtopic': get_subtopic(soup),\n",
    "        'Repository Name': get_repo_names(soup),\n",
    "        'User Name': get_user_names(soup),\n",
    "        'Star Ratings': get_star_ratings(soup)\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    repositories_df = pd.DataFrame.from_dict(repositories_dict)\n",
    "\n",
    "    return repositories_df\n",
    "\n",
    "# Helper functions to extract data from individual repository pages\n",
    "def get_subtopic(soup):\n",
    "    # Add code to extract subtopic information\n",
    "    pass\n",
    "\n",
    "def get_repo_names(soup):\n",
    "    # Add code to extract repository names\n",
    "    pass\n",
    "\n",
    "def get_user_names(soup):\n",
    "    # Add code to extract user names\n",
    "    pass\n",
    "\n",
    "def get_star_ratings(soup):\n",
    "    # Add code to extract star ratings\n",
    "    pass\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = 'data'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file for each topic\n",
    "topics_data = scrape_topics()\n",
    "for index, row in topics_data.iterrows():\n",
    "    # Use the topic title as the CSV file name\n",
    "    file_name = row['Title'].lower().replace(' ', '_') + '_repositories.csv'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    row['Repositories'].to_csv(file_path, index=False)\n",
    "    print(f'DataFrame for {row[\"Title\"]} saved to {file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d091a51-6108-420d-b230-0d33192e1b94",
   "metadata": {},
   "source": [
    "## Feel free to edit and modify. I am here to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd78fee-06ea-444a-9132-b293d2850a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
